{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:400px; height:250px; border-bottom:10px black solid;border-left: 10px green solid;padding: 10px 10px 10px 10px;'>\n",
    "    <h1> ---Timeseries forcasting of the Norwegian salmon exportation---</h1>\n",
    "    <h3> Author: Jakob Lindstrøm, Joakim Sælemyr, Henrik Knudsen, Marcus Hjertaas </h3>\n",
    "    <h3> Date: 29.05.2023 </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:600px; height:950px; padding: 10px 10px 10px 10px;border-bottom:10px black solid;\n",
    "            border-left: 10px green solid'>\n",
    "    <div>\n",
    "        <h4>Keywords:</h4>\n",
    "        <p> Salmon, timeseries, SARIMAX </p>\n",
    "        <h4> Data sources:  </h4>\n",
    "        <p> MET: https://api.met.no/ </p>\n",
    "        <p> geonorge: https://www.geonorge.no/ </p>\n",
    "        <h4> Intention: </h4>\n",
    "        <p> This programe is a modified version of the coding part for me and my fellow students when we wrote our bachelor thesis. This modified version separates itself from what we delivered in the matter that I removed a lot of the exploratory dataframe analysis, I have merge all our notebooks into the same notebook, I have cleaned some code and added a fifth classification model: Naive Bayes. </p>\n",
    "        <div>\n",
    "            <h2>Executive summary </h2>\n",
    "            <p>\n",
    "            The purpose of this bachelor thesis is to find the best machine learning method to accurately \n",
    "            predict bank term deposits from the public. We are viewing the issue from a business \n",
    "            perspective, also taking real-world implications into account.  \n",
    "                <br><br>\n",
    "            The data which the models are applied to provides a series of different information from a \n",
    "            sample of Portuguese individuals, including whether or not these individuals have made \n",
    "            committed long-term deposits into the bank. The data have gone through a set of \n",
    "            configurations and resampling techniques. Retrieval of the data was in the wake of the Great \n",
    "            Financial Crisis of 2008. \n",
    "                <br><br>            \n",
    "            Four machine learning models were chosen for prediction, they are all based on \n",
    "            acknowledged statistical classification methods, these are: binomial logistic regression, \n",
    "            decision tree classifier, artificial neural networks, and support vector machine. These methods \n",
    "            vary in complexity and have different sets of advantages and disadvantages. They will be \n",
    "            applied to face the prediction-challenges in the data. It’s not possible to combine one method \n",
    "            to cover another method's disadvantages. However, by using several methods it is believed \n",
    "            that one can find the model that best faces the specific challenges provided by the data. \n",
    "            The best model is based on a comprehensive assessment, including two criterions. The first \n",
    "            criterion is based on the prediction rate and the model’s ability at predicting actual possible \n",
    "            customers. The second criterion concerns the model's training data and its amount of \n",
    "            resampling interference. \n",
    "                <br><br>\n",
    "                The models are configured to give predictions for the conditions during the period of data\u0002retrieval and are therefore constrained to these conditions. According to our findings, the \n",
    "            support vector machine model trained on the undersampled data is the most favorable model. \n",
    "            The model showed both great prediction accuracy and managed to classify the minority class \n",
    "            precisely.\n",
    "            </p>\n",
    "        </div>   \n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Importations of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score as cvs\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from seaborn import heatmap as hm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier as dtc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Resampling </h2>\n",
    "<p> Splitting the dataframe into a test set and three train sets. The three train sets are based on different resampling techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('original.csv')\n",
    "del df['Unnamed: 0']\n",
    "\n",
    "#Splitting into original train set and universal test size\n",
    "orgtrain, test = train_test_split(df, test_size=.2, random_state=42)\n",
    "\n",
    "#Retriving all 1s from original train set\n",
    "ones = orgtrain.loc[orgtrain.Deposit == 1]\n",
    "\n",
    "#Retriving random zeros from the original train set\n",
    "alfa, randomzeros = train_test_split(orgtrain.loc[orgtrain.Deposit==0], test_size=.13255, random_state=42)\n",
    "\n",
    "under = pd.concat([ones, randomzeros], axis=0)\n",
    "\n",
    "#Retrieving all 0s from original train set\n",
    "allzeros = orgtrain.loc[orgtrain.Deposit == 0]\n",
    "\n",
    "#Replicating all the ones from original train set seven times,\n",
    "#and concats it into one df\n",
    "newone = ones\n",
    "for i in range(0,6,1):\n",
    "    newone = pd.concat([newone,ones],axis=0)\n",
    "\n",
    "#Retrieving random 1s from original train set \n",
    "#so the amount of 1s and 0s is matching.\n",
    "charlie, additionalones = train_test_split(ones, test_size=0.54429, random_state=42)\n",
    "\n",
    "#adding all the ones to create one large df with only 1s.\n",
    "totalones = pd.concat([newone,additionalones], axis=0)\n",
    "\n",
    "#Concating the 0s and 1s into one oversample df.\n",
    "over = pd.concat([totalones, allzeros],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading csv-files\n",
    "# test = pd.read_csv('test.csv')\n",
    "# orgtrain = pd.read_csv('org.csv')\n",
    "# over  = pd.read_csv('over.csv')\n",
    "# under = pd.read_csv('under.csv')\n",
    "# del test['Unnamed: 0.1']\n",
    "# del test['Unnamed: 0']\n",
    "# del orgtrain['Unnamed: 0']\n",
    "# del over['Unnamed: 0']\n",
    "# del under['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Specifying targets and data to be used in the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column  = test.columns.values.tolist()\n",
    "column.remove('Deposit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtarget = test[['Deposit']]\n",
    "testdata = test[column]\n",
    "\n",
    "org_train_target= orgtrain[['Deposit']]\n",
    "org_train_data= orgtrain[column]\n",
    "\n",
    "under_train_target = under[['Deposit']]\n",
    "under_train_data = under[column]\n",
    "\n",
    "over_train_target = over[['Deposit']]\n",
    "over_train_data= over[column]\n",
    "\n",
    "datas = [org_train_data,under_train_data,over_train_data,testdata]\n",
    "targets = [org_train_target,under_train_target,over_train_target,testtarget]\n",
    "idx = ['Original', 'Under','Over']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jakob\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Jakob\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Jakob\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "def LogReg():\n",
    "    model = LogisticRegression(solver='lbfgs',max_iter=95000 )         \n",
    "    \n",
    "\n",
    "    train_res = []\n",
    "    test_res = []\n",
    "    pcl_res = []\n",
    "\n",
    "    for i in range(0,3,1):\n",
    "        model = model.fit(datas[i],targets[i])\n",
    "        train_res_sel = model.score(datas[i],targets[i])\n",
    "        test_res_sel = model.score(testdata,testtarget)\n",
    "        cm = confusion_matrix(model.predict(testdata),testtarget) \n",
    "        cm = cm/np.sum(cm)\n",
    "        cm = cm[0][1]/(cm[0][1]+cm[0][0]) \n",
    "               \n",
    "        train_res.append(train_res_sel)\n",
    "        test_res.append(test_res_sel)\n",
    "        pcl_res.append(cm)\n",
    "\n",
    "    df = pd.DataFrame({'Train':train_res,'Test':test_res,'PCL':pcl_res})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiBay():\n",
    "    model = GaussianNB()         \n",
    "\n",
    "    train_res = []\n",
    "    test_res = []\n",
    "    pcl_res = []\n",
    "\n",
    "    for i in range(0,3,1):\n",
    "        model = model.fit(datas[i],targets[i])\n",
    "        train_res_sel = model.score(datas[i],targets[i])\n",
    "        test_res_sel = model.score(testdata,testtarget)\n",
    "        cm = confusion_matrix(model.predict(testdata),testtarget)\n",
    "        cm = cm/np.sum(cm)\n",
    "        cm = cm[0][1]/(cm[0][1]+cm[0][0]) \n",
    "               \n",
    "        train_res.append(train_res_sel)\n",
    "        test_res.append(test_res_sel)\n",
    "        pcl_res.append(cm)\n",
    "                \n",
    "#         cm = confusion_matrix(model.predict(datas[i]), targets[i])\n",
    "#         cm = cm/np.sum(cm)\n",
    "#         plt.figure(figsize=(6,6))\n",
    "#         sns.heatmap(cm, annot=True,fmt='0.3%', cmap='Blues')\n",
    "#         plt.ylabel(\"Actual\")\n",
    "#         plt.xlabel(\"Predicted\")\n",
    "\n",
    "    df = pd.DataFrame({'Train':train_res,'Test':test_res,'PCL':pcl_res})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Artifical Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ANN():\n",
    "    for i in range(0,3,1):\n",
    "        # Scaling the numeric variables\n",
    "        num_vars = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "        for i in range(0,4,1):\n",
    "            scaler = MinMaxScaler()\n",
    "            datas[i][num_vars] = scaler.fit_transform(datas[i][num_vars])\n",
    "\n",
    "\n",
    "    train_res = []\n",
    "    test_res = []\n",
    "    pcl_res = []\n",
    "\n",
    "    for i in range(0,3,1):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=datas[i].shape[1], activation=\"leaky_relu\"))\n",
    "        model.add(Dense(16, activation=\"leaky_relu\"))\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        # Train the model\n",
    "        model.fit(x=datas[i], y=targets[i], epochs=20, batch_size=10, verbose=1, validation_split=.1)\n",
    "\n",
    "        # Train \n",
    "        predictions = model.predict(x=datas[i], batch_size=10, verbose=0)\n",
    "        rounded_pred = [np.round(num) for num in predictions]\n",
    "        cm = confusion_matrix(rounded_pred,targets[i])\n",
    "        cm = cm/np.sum(cm)\n",
    "        train_res_sel = cm[0][0] + cm[1][1]\n",
    "\n",
    "        # Test and CLP\n",
    "        predictions_test = model.predict(x=testdata, batch_size=10, verbose=0)\n",
    "        rounded_pred_test = [np.round(num) for num in predictions_test]\n",
    "        cm = confusion_matrix(rounded_pred_test,testtarget)\n",
    "        cm = cm/np.sum(cm)\n",
    "        test_res_sel = cm[0][0] +  cm[1][1]\n",
    "        cm = cm[0][1]/(cm[0][1]+cm[0][0]) \n",
    "\n",
    "        train_res.append(train_res_sel)\n",
    "        test_res.append(test_res_sel)\n",
    "        pcl_res.append(cm)\n",
    "\n",
    "    df = pd.DataFrame({'Train':train_res,'Test':test_res,'PCL':pcl_res})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTC():\n",
    "    \n",
    "    train_res = []\n",
    "    test_res = []\n",
    "    pcl_res = []\n",
    "    \n",
    "    for i in range(0,3,1):\n",
    "        model = dtc().fit(datas[i], targets[i])\n",
    "\n",
    "        path = model.cost_complexity_pruning_path(datas[i], targets[i])\n",
    "        ccp_alphas = path.ccp_alphas\n",
    "        ccp_alphas = ccp_alphas[:-1]\n",
    "        \n",
    "        model = dtc()\n",
    "        scores = cvs(model, datas[i], targets[i], cv=2)\n",
    "\n",
    "        alpha_loop_values = []\n",
    "        for alpha in ccp_alphas:\n",
    "            model = dtc(ccp_alpha=alpha, random_state=42)\n",
    "            scores = cvs(model, datas[i], targets[i], cv=5)\n",
    "            alpha_loop_values.append([alpha, np.mean(scores), np.std(scores)])\n",
    "\n",
    "        alpha_results = pd.DataFrame(alpha_loop_values, columns = [\"Alpha\", \"Mean_Accuracy\", \"STD_Accuracy\"])\n",
    "\n",
    "        # Selecting the optimal alpha, based on maximum mean accuracy\n",
    "        ideal_alpha = alpha_results[alpha_results.Mean_Accuracy == max(alpha_results.Mean_Accuracy)].Alpha\n",
    "        if len(ideal_alpha) > 1:\n",
    "            ideal_alpha = ideal_alpha.iloc[-1]\n",
    "        ideal_alpha = float(ideal_alpha)\n",
    "\n",
    "        model = dtc(ccp_alpha = ideal_alpha)\n",
    "        model = model.fit(datas[i], targets[i])\n",
    "\n",
    "        train_res_sel = model.score(datas[i],targets[i])\n",
    "        test_res_sel = model.score(testdata,testtarget)\n",
    "        cm = confusion_matrix(model.predict(testdata),testtarget)\n",
    "        cm = cm/np.sum(cm)\n",
    "        cm = cm[0][1]/(cm[0][1]+cm[0][0]) \n",
    "               \n",
    "        train_res.append(train_res_sel)\n",
    "        test_res.append(test_res_sel)\n",
    "        pcl_res.append(cm)\n",
    "\n",
    "    df = pd.DataFrame({'Train':train_res,'Test':test_res,'PCL':pcl_res})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM():\n",
    "\n",
    "    train_res = []\n",
    "    test_res = []\n",
    "    pcl_res = []\n",
    "\n",
    "    for i in range(0,3,1):\n",
    "\n",
    "        datas_scaled = scale(datas[i])\n",
    "        targets_scaled = scale(targets[i])\n",
    "\n",
    "        model = SVC(random_state=42, C=5, gamma=0.01)\n",
    "        model = model.fit(datas_scaled, targets[i])\n",
    "\n",
    "        train_res_sel = model.score(datas_scaled,targets[i])\n",
    "        test_res_sel = model.score(scale(datas[3]),testtarget)\n",
    "        cm = confusion_matrix(model.predict(scale(testdata)),testtarget)\n",
    "        cm = cm/np.sum(cm)\n",
    "        cm = cm[0][1]/(cm[0][1]+cm[0][0]) \n",
    "               \n",
    "        train_res.append(train_res_sel)\n",
    "        test_res.append(test_res_sel)\n",
    "        pcl_res.append(cm)\n",
    "\n",
    "    df = pd.DataFrame({'Train':train_res,'Test':test_res,'PCL':pcl_res})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Retrieving the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>PCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.902096</td>\n",
       "      <td>0.894946</td>\n",
       "      <td>0.062812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under</th>\n",
       "      <td>0.832859</td>\n",
       "      <td>0.813668</td>\n",
       "      <td>0.022696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Over</th>\n",
       "      <td>0.832702</td>\n",
       "      <td>0.759372</td>\n",
       "      <td>0.014948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Train      Test       PCL\n",
       "Original  0.902096  0.894946  0.062812\n",
       "Under     0.832859  0.813668  0.022696\n",
       "Over      0.832702  0.759372  0.014948"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogRegres = LogReg()\n",
    "LogRegres = LogRegres.set_index(pd.Series(idx))\n",
    "LogRegres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>PCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.862282</td>\n",
       "      <td>0.835674</td>\n",
       "      <td>0.053453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under</th>\n",
       "      <td>0.702584</td>\n",
       "      <td>0.842420</td>\n",
       "      <td>0.061416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Over</th>\n",
       "      <td>0.711664</td>\n",
       "      <td>0.811235</td>\n",
       "      <td>0.046741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Train      Test       PCL\n",
       "Original  0.862282  0.835674  0.053453\n",
       "Under     0.702584  0.842420  0.061416\n",
       "Over      0.711664  0.811235  0.046741"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NaiBayres = NaiBay()\n",
    "NaiBayres = NaiBayres.set_index(pd.Series(idx))\n",
    "NaiBayres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.2714 - accuracy: 0.8946 - val_loss: 0.2298 - val_accuracy: 0.9057\n",
      "Epoch 2/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.2285 - accuracy: 0.9029 - val_loss: 0.2193 - val_accuracy: 0.9066\n",
      "Epoch 3/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.2210 - accuracy: 0.9034 - val_loss: 0.2118 - val_accuracy: 0.9082\n",
      "Epoch 4/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.2164 - accuracy: 0.9045 - val_loss: 0.2137 - val_accuracy: 0.9068\n",
      "Epoch 5/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.2131 - accuracy: 0.9061 - val_loss: 0.2070 - val_accuracy: 0.9135\n",
      "Epoch 6/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.2092 - accuracy: 0.9059 - val_loss: 0.2186 - val_accuracy: 0.9082\n",
      "Epoch 7/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.2069 - accuracy: 0.9065 - val_loss: 0.2029 - val_accuracy: 0.9118\n",
      "Epoch 8/20\n",
      "3256/3256 [==============================] - 5s 1ms/step - loss: 0.2050 - accuracy: 0.9080 - val_loss: 0.2031 - val_accuracy: 0.9118\n",
      "Epoch 9/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.2023 - accuracy: 0.9084 - val_loss: 0.2031 - val_accuracy: 0.9104\n",
      "Epoch 10/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.2006 - accuracy: 0.9085 - val_loss: 0.2060 - val_accuracy: 0.9074\n",
      "Epoch 11/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.1987 - accuracy: 0.9088 - val_loss: 0.1982 - val_accuracy: 0.9104\n",
      "Epoch 12/20\n",
      "3256/3256 [==============================] - 5s 2ms/step - loss: 0.1975 - accuracy: 0.9105 - val_loss: 0.2028 - val_accuracy: 0.9115\n",
      "Epoch 13/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.1961 - accuracy: 0.9114 - val_loss: 0.2069 - val_accuracy: 0.9071\n",
      "Epoch 14/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.1963 - accuracy: 0.9104 - val_loss: 0.2013 - val_accuracy: 0.9074\n",
      "Epoch 15/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.1946 - accuracy: 0.9118 - val_loss: 0.2019 - val_accuracy: 0.9093\n",
      "Epoch 16/20\n",
      "3256/3256 [==============================] - 5s 1ms/step - loss: 0.1935 - accuracy: 0.9118 - val_loss: 0.1992 - val_accuracy: 0.9113\n",
      "Epoch 17/20\n",
      "3256/3256 [==============================] - 5s 2ms/step - loss: 0.1923 - accuracy: 0.9124 - val_loss: 0.2110 - val_accuracy: 0.9104\n",
      "Epoch 18/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.1920 - accuracy: 0.9126 - val_loss: 0.1997 - val_accuracy: 0.9082\n",
      "Epoch 19/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.1913 - accuracy: 0.9122 - val_loss: 0.1987 - val_accuracy: 0.9101\n",
      "Epoch 20/20\n",
      "3256/3256 [==============================] - 4s 1ms/step - loss: 0.1901 - accuracy: 0.9126 - val_loss: 0.1985 - val_accuracy: 0.9126\n",
      "Epoch 1/20\n",
      "760/760 [==============================] - 3s 4ms/step - loss: 0.5314 - accuracy: 0.7379 - val_loss: 0.4262 - val_accuracy: 0.8389\n",
      "Epoch 2/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.4037 - accuracy: 0.8359 - val_loss: 0.4174 - val_accuracy: 0.8377\n",
      "Epoch 3/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.3748 - accuracy: 0.8446 - val_loss: 0.4934 - val_accuracy: 0.7915\n",
      "Epoch 4/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.3627 - accuracy: 0.8535 - val_loss: 0.4374 - val_accuracy: 0.8187\n",
      "Epoch 5/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.3559 - accuracy: 0.8566 - val_loss: 0.4746 - val_accuracy: 0.7903\n",
      "Epoch 6/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.3477 - accuracy: 0.8587 - val_loss: 0.4183 - val_accuracy: 0.8092\n",
      "Epoch 7/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.3431 - accuracy: 0.8622 - val_loss: 0.5214 - val_accuracy: 0.7666\n",
      "Epoch 8/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.3379 - accuracy: 0.8645 - val_loss: 0.4732 - val_accuracy: 0.7903\n",
      "Epoch 9/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.3351 - accuracy: 0.8633 - val_loss: 0.4243 - val_accuracy: 0.8104\n",
      "Epoch 10/20\n",
      "760/760 [==============================] - 1s 996us/step - loss: 0.3285 - accuracy: 0.8697 - val_loss: 0.4266 - val_accuracy: 0.8021\n",
      "Epoch 11/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.3241 - accuracy: 0.8691 - val_loss: 0.3684 - val_accuracy: 0.8318\n",
      "Epoch 12/20\n",
      "760/760 [==============================] - 1s 2ms/step - loss: 0.3218 - accuracy: 0.8713 - val_loss: 0.4743 - val_accuracy: 0.7749\n",
      "Epoch 13/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.3169 - accuracy: 0.8745 - val_loss: 0.4048 - val_accuracy: 0.8116\n",
      "Epoch 14/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.3142 - accuracy: 0.8718 - val_loss: 0.4569 - val_accuracy: 0.7855\n",
      "Epoch 15/20\n",
      "760/760 [==============================] - 1s 873us/step - loss: 0.3107 - accuracy: 0.8749 - val_loss: 0.4922 - val_accuracy: 0.7678\n",
      "Epoch 16/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.3082 - accuracy: 0.8768 - val_loss: 0.4267 - val_accuracy: 0.7998\n",
      "Epoch 17/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.3046 - accuracy: 0.8768 - val_loss: 0.4850 - val_accuracy: 0.7701\n",
      "Epoch 18/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.3020 - accuracy: 0.8771 - val_loss: 0.4141 - val_accuracy: 0.8033\n",
      "Epoch 19/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.2997 - accuracy: 0.8783 - val_loss: 0.4623 - val_accuracy: 0.7867\n",
      "Epoch 20/20\n",
      "760/760 [==============================] - 1s 1ms/step - loss: 0.2984 - accuracy: 0.8776 - val_loss: 0.4240 - val_accuracy: 0.7998\n",
      "Epoch 1/20\n",
      "5728/5728 [==============================] - 8s 1ms/step - loss: 0.4004 - accuracy: 0.8241 - val_loss: 0.4056 - val_accuracy: 0.8123\n",
      "Epoch 2/20\n",
      "5728/5728 [==============================] - 7s 1ms/step - loss: 0.3385 - accuracy: 0.8610 - val_loss: 0.4157 - val_accuracy: 0.7998\n",
      "Epoch 3/20\n",
      "5728/5728 [==============================] - 7s 1ms/step - loss: 0.3237 - accuracy: 0.8679 - val_loss: 0.3727 - val_accuracy: 0.8207\n",
      "Epoch 4/20\n",
      "5728/5728 [==============================] - 6s 1ms/step - loss: 0.3129 - accuracy: 0.8729 - val_loss: 0.4829 - val_accuracy: 0.7720\n",
      "Epoch 5/20\n",
      "5728/5728 [==============================] - 6s 1ms/step - loss: 0.3069 - accuracy: 0.8756 - val_loss: 0.4267 - val_accuracy: 0.7956\n",
      "Epoch 6/20\n",
      "5728/5728 [==============================] - 7s 1ms/step - loss: 0.3008 - accuracy: 0.8801 - val_loss: 0.3088 - val_accuracy: 0.8415\n",
      "Epoch 7/20\n",
      "5728/5728 [==============================] - 6s 1ms/step - loss: 0.2979 - accuracy: 0.8813 - val_loss: 0.3229 - val_accuracy: 0.8393\n",
      "Epoch 8/20\n",
      "5728/5728 [==============================] - 6s 1ms/step - loss: 0.2935 - accuracy: 0.8815 - val_loss: 0.4310 - val_accuracy: 0.7980\n",
      "Epoch 9/20\n",
      "5728/5728 [==============================] - 5s 945us/step - loss: 0.2895 - accuracy: 0.8842 - val_loss: 0.4099 - val_accuracy: 0.8053\n",
      "Epoch 10/20\n",
      "5728/5728 [==============================] - 6s 1ms/step - loss: 0.2875 - accuracy: 0.8852 - val_loss: 0.3864 - val_accuracy: 0.8174\n",
      "Epoch 11/20\n",
      "5728/5728 [==============================] - 6s 974us/step - loss: 0.2846 - accuracy: 0.8868 - val_loss: 0.3764 - val_accuracy: 0.8244\n",
      "Epoch 12/20\n",
      "5728/5728 [==============================] - 6s 1ms/step - loss: 0.2828 - accuracy: 0.8873 - val_loss: 0.3734 - val_accuracy: 0.8212\n",
      "Epoch 13/20\n",
      "5728/5728 [==============================] - 6s 1ms/step - loss: 0.2800 - accuracy: 0.8893 - val_loss: 0.4227 - val_accuracy: 0.8006\n",
      "Epoch 14/20\n",
      "5728/5728 [==============================] - 7s 1ms/step - loss: 0.2791 - accuracy: 0.8890 - val_loss: 0.3551 - val_accuracy: 0.8324\n",
      "Epoch 15/20\n",
      "5728/5728 [==============================] - 6s 1ms/step - loss: 0.2772 - accuracy: 0.8918 - val_loss: 0.4810 - val_accuracy: 0.7879\n",
      "Epoch 16/20\n",
      "5728/5728 [==============================] - 6s 1ms/step - loss: 0.2753 - accuracy: 0.8917 - val_loss: 0.3831 - val_accuracy: 0.8217\n",
      "Epoch 17/20\n",
      "5728/5728 [==============================] - 6s 1ms/step - loss: 0.2742 - accuracy: 0.8919 - val_loss: 0.3403 - val_accuracy: 0.8396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      "5728/5728 [==============================] - 7s 1ms/step - loss: 0.2725 - accuracy: 0.8935 - val_loss: 0.4169 - val_accuracy: 0.8061\n",
      "Epoch 19/20\n",
      "5728/5728 [==============================] - 6s 1ms/step - loss: 0.2704 - accuracy: 0.8931 - val_loss: 0.4510 - val_accuracy: 0.7904\n",
      "Epoch 20/20\n",
      "5728/5728 [==============================] - 6s 1ms/step - loss: 0.2691 - accuracy: 0.8950 - val_loss: 0.3474 - val_accuracy: 0.8368\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>PCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.915035</td>\n",
       "      <td>0.881013</td>\n",
       "      <td>0.037281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under</th>\n",
       "      <td>0.876956</td>\n",
       "      <td>0.771868</td>\n",
       "      <td>0.012903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Over</th>\n",
       "      <td>0.891029</td>\n",
       "      <td>0.732500</td>\n",
       "      <td>0.010441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Train      Test       PCL\n",
       "Original  0.915035  0.881013  0.037281\n",
       "Under     0.876956  0.771868  0.012903\n",
       "Over      0.891029  0.732500  0.010441"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANNres = ANN()\n",
    "ANNres = ANNres.set_index(pd.Series(idx))\n",
    "ANNres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>PCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.905607</td>\n",
       "      <td>0.867522</td>\n",
       "      <td>0.048409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under</th>\n",
       "      <td>0.870673</td>\n",
       "      <td>0.767113</td>\n",
       "      <td>0.020820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Over</th>\n",
       "      <td>0.999890</td>\n",
       "      <td>0.844299</td>\n",
       "      <td>0.091511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Train      Test       PCL\n",
       "Original  0.905607  0.867522  0.048409\n",
       "Under     0.870673  0.767113  0.020820\n",
       "Over      0.999890  0.844299  0.091511"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTCres = DTC()\n",
    "DTCres = DTCres.set_index(pd.Series(idx))\n",
    "DTCres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jakob\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Jakob\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Jakob\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>PCL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.921063</td>\n",
       "      <td>0.902687</td>\n",
       "      <td>0.082132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under</th>\n",
       "      <td>0.888454</td>\n",
       "      <td>0.696340</td>\n",
       "      <td>0.009238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Over</th>\n",
       "      <td>0.900787</td>\n",
       "      <td>0.732279</td>\n",
       "      <td>0.022778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Train      Test       PCL\n",
       "Original  0.921063  0.902687  0.082132\n",
       "Under     0.888454  0.696340  0.009238\n",
       "Over      0.900787  0.732279  0.022778"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVMres =  SVM()\n",
    "SVMres = SVMres.set_index(pd.Series(idx))\n",
    "SVMres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Combining the results into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [LogRegres, NaiBayres,ANNres, DTCres, SVMres]\n",
    "methods = ['Logistic','Naive  Bayes','ANN','DTC','SVM']\n",
    "datasets = ['Original', 'Under','Over']\n",
    "\n",
    "for i in range(0,len(results),1):\n",
    "    labels = [np.repeat(methods[i],3),datasets]\n",
    "    tuples = list(zip(*labels))\n",
    "    index = pd.MultiIndex.from_tuples(tuples, names=['Method', 'Dataset'])\n",
    "    results[i] = results[i].set_index(index)\n",
    "resultsdf = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "      <th>PCL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Logistic</th>\n",
       "      <th>Original</th>\n",
       "      <td>0.902096</td>\n",
       "      <td>0.894946</td>\n",
       "      <td>0.062812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under</th>\n",
       "      <td>0.832859</td>\n",
       "      <td>0.813668</td>\n",
       "      <td>0.022696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Over</th>\n",
       "      <td>0.832702</td>\n",
       "      <td>0.759372</td>\n",
       "      <td>0.014948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Naive  Bayes</th>\n",
       "      <th>Original</th>\n",
       "      <td>0.862282</td>\n",
       "      <td>0.835674</td>\n",
       "      <td>0.053453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under</th>\n",
       "      <td>0.702584</td>\n",
       "      <td>0.842420</td>\n",
       "      <td>0.061416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Over</th>\n",
       "      <td>0.711664</td>\n",
       "      <td>0.811235</td>\n",
       "      <td>0.046741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">ANN</th>\n",
       "      <th>Original</th>\n",
       "      <td>0.915035</td>\n",
       "      <td>0.881013</td>\n",
       "      <td>0.037281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under</th>\n",
       "      <td>0.876956</td>\n",
       "      <td>0.771868</td>\n",
       "      <td>0.012903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Over</th>\n",
       "      <td>0.891029</td>\n",
       "      <td>0.732500</td>\n",
       "      <td>0.010441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">DTC</th>\n",
       "      <th>Original</th>\n",
       "      <td>0.905607</td>\n",
       "      <td>0.867522</td>\n",
       "      <td>0.048409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under</th>\n",
       "      <td>0.870673</td>\n",
       "      <td>0.767113</td>\n",
       "      <td>0.020820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Over</th>\n",
       "      <td>0.999890</td>\n",
       "      <td>0.844299</td>\n",
       "      <td>0.091511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">SVM</th>\n",
       "      <th>Original</th>\n",
       "      <td>0.921063</td>\n",
       "      <td>0.902687</td>\n",
       "      <td>0.082132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under</th>\n",
       "      <td>0.888454</td>\n",
       "      <td>0.696340</td>\n",
       "      <td>0.009238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Over</th>\n",
       "      <td>0.900787</td>\n",
       "      <td>0.732279</td>\n",
       "      <td>0.022778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Train      Test       PCL\n",
       "Method       Dataset                               \n",
       "Logistic     Original  0.902096  0.894946  0.062812\n",
       "             Under     0.832859  0.813668  0.022696\n",
       "             Over      0.832702  0.759372  0.014948\n",
       "Naive  Bayes Original  0.862282  0.835674  0.053453\n",
       "             Under     0.702584  0.842420  0.061416\n",
       "             Over      0.711664  0.811235  0.046741\n",
       "ANN          Original  0.915035  0.881013  0.037281\n",
       "             Under     0.876956  0.771868  0.012903\n",
       "             Over      0.891029  0.732500  0.010441\n",
       "DTC          Original  0.905607  0.867522  0.048409\n",
       "             Under     0.870673  0.767113  0.020820\n",
       "             Over      0.999890  0.844299  0.091511\n",
       "SVM          Original  0.921063  0.902687  0.082132\n",
       "             Under     0.888454  0.696340  0.009238\n",
       "             Over      0.900787  0.732279  0.022778"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Final remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> Our conclusion was that the model for the support vector machine was the best model for our business problem. The main reason for this was due to fairly good overall scores on the train and test set, and it had the lowest PCL-score. PCL score is a parameter that measures the amount of times our model predicts true-Negatives. We decided that this was an important measurment seen from a business perspective and therefore the model based on the support vector machine algorithm and trained on the undersampled dataset had best results. For more informmation; read our thesis (its in the repository)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
